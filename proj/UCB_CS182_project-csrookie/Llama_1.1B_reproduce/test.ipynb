{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled0.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1J0nRtRnRbNiPlDiVxOQcGqhBl0xtBTUX\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    # BitsAndBytesConfig, # <-- 已移除\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "# 【已修复】导入 PeftModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "HOTPOT_DATASET_NAME = \"hotpot_qa\"\n",
    "HOTPOT_DATASET_CONFIG = \"distractor\"\n",
    "MATH_DATASET_NAME = \"qwedsacf/competition_math\"\n",
    "RESULTS_DIR = \"./drive/MyDrive/\"\n",
    "\n",
    "# 【检查点路径 1 - 对照组】\n",
    "JOINT_ADAPTER_PATH = os.path.join(RESULTS_DIR, \"joint_adapter_llama_fp32\")\n",
    "\n",
    "# 【检查点路径 2 - 实验组 Phase 1 (MATH)】\n",
    "TASK_B_ADAPTER_PATH = os.path.join(RESULTS_DIR, \"math_adapter_llama_fp32\")\n",
    "\n",
    "\n",
    "# --- VRAM-Saving Config ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "# 【BUG 修复】降低 BS 以适应 FP32\n",
    "PER_DEVICE_BS = 64\n",
    "GRAD_ACC_STEPS = 1 # (有效批量大小仍然是 8 * 4 = 32)\n",
    "\n",
    "# --- Experiment Config ---\n",
    "N_TRAIN_EXAMPLES = 4000\n",
    "N_VAL_EXAMPLES = 400\n",
    "JOINT_EPOCHS = 2\n",
    "TASK_A_EPOCHS = 2 # 用于 Phase 2 (HotpotQA)\n",
    "TASK_B_EPOCHS = 1 # 用于 Phase 1 (MATH)\n",
    "\n",
    "# --- 2. Utility Functions (Data Formatting - Llama Chat Style) ---\n",
    "def format_hotpot_qa(example):\n",
    "    \"\"\"Formats HotpotQA data into a Llama-chat-style prompt.\"\"\"\n",
    "    context = \" \".join([\"\".join(s) for s in example[\"context\"][\"sentences\"]])\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    text = (\n",
    "        f\"<s>[INST] You are a helpful assistant. Use the following context to \"\n",
    "        f\"answer the question. Context: {context}\\n\\nQuestion: {question} [/INST] \"\n",
    "        f\"Answer: {answer}</s>\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def format_math(example):\n",
    "    \"\"\"Formats MATH data into a Llama-chat-style prompt.\"\"\"\n",
    "    problem = example[\"problem\"]\n",
    "    solution = example[\"solution\"]\n",
    "\n",
    "    text = (\n",
    "        f\"<s>[INST] You are a math expert. Solve the following math problem. \"\n",
    "        f\"Show your work.\\nProblem: {problem} [/INST] \"\n",
    "        f\"Solution: {solution}</s>\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def filter_by_length(example, tokenizer, formatter):\n",
    "    \"\"\"\n",
    "    只检查长度。返回 True (保留) 或 False (丢弃)。\n",
    "    \"\"\"\n",
    "    text = formatter(example)\n",
    "    tokenized = tokenizer(text, max_length=MAX_SEQ_LENGTH + 1, truncation=False, padding=False)\n",
    "    return len(tokenized['input_ids']) <= MAX_SEQ_LENGTH\n",
    "\n",
    "# 【BUG 修复】这是修复了 1.7 Loss 问题 和 ValueError 的 Preprocess 函数\n",
    "def preprocess(example, tokenizer, formatter):\n",
    "    \"\"\"\n",
    "    【已修正】\n",
    "    格式化文本，应用损失掩码，并填充到最大长度。\n",
    "    \"\"\"\n",
    "    text = formatter(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # 修复 ValueError\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    inst_token_id = tokenizer.convert_tokens_to_ids(\"]\")\n",
    "\n",
    "    split_point = -1\n",
    "    for i in range(len(tokenized[\"input_ids\"]) - 1, -1, -1):\n",
    "        if tokenized[\"input_ids\"][i] == inst_token_id:\n",
    "            split_point = i + 1\n",
    "            break\n",
    "\n",
    "    if split_point == -1:\n",
    "        return {}\n",
    "\n",
    "    for i in range(split_point):\n",
    "        labels[i] = -100\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# --- 3. Model Loading (【重构】) ---\n",
    "\n",
    "def get_model_and_tokenizer_base():\n",
    "    \"\"\"\n",
    "    只加载 FP16 TinyLlama 基础模型和 Tokenizer。\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16, # <-- 加载时仍然用 FP16 (节省 RAM)，但训练会是 FP32\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # 在基础模型上启用梯度检查点\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_lora_config():\n",
    "    \"\"\"\n",
    "    只定义 LoRA 配置。\n",
    "    \"\"\"\n",
    "    return LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "def manual_evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    在给定 dataloader 上手动运行评估。\n",
    "    \"\"\"\n",
    "    model.eval()  # <--- 设置为评估模式\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    with torch.no_grad(): # <--- 禁用梯度计算\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # 将批次移动到模型所在的设备\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "    model.train() # <--- 【重要】将模型设置回训练模式\n",
    "    return total_loss / total_steps\n",
    "\n",
    "# --- 4. Main Experiment Logic ---\n",
    "def main():\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "\n",
    "    print(f\"--- Loading Base Model & Tokenizer ---\")\n",
    "    base_model, tokenizer = get_model_and_tokenizer_base()\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # --- Load and Process Datasets ---\n",
    "    print(f\"\\n--- Loading and Preprocessing Datasets (This may take a while) ---\")\n",
    "\n",
    "    # Task A: HotpotQA\n",
    "    raw_hotpot = load_dataset(HOTPOT_DATASET_NAME, HOTPOT_DATASET_CONFIG)\n",
    "    hotpot_train = raw_hotpot[\"train\"].shuffle(seed=42).select(range(N_TRAIN_EXAMPLES))\n",
    "    hotpot_val = raw_hotpot[\"validation\"].shuffle(seed=42).select(range(N_VAL_EXAMPLES))\n",
    "\n",
    "    print(f\"Tokenizing and filtering HotpotQA...\")\n",
    "    hotpot_train_tokenized = hotpot_train.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    hotpot_val_tokenized = hotpot_val.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    print(f\"HotpotQA: {len(hotpot_train_tokenized)} train, {len(hotpot_val_tokenized)} val (after filtering)\")\n",
    "\n",
    "    # Task B: MATH\n",
    "    raw_math = load_dataset(MATH_DATASET_NAME)\n",
    "    total_math_samples_needed = N_TRAIN_EXAMPLES + N_VAL_EXAMPLES\n",
    "    math_subset = raw_math[\"train\"].shuffle(seed=42).select(range(total_math_samples_needed))\n",
    "    val_size_fraction = N_VAL_EXAMPLES / total_math_samples_needed\n",
    "    math_splits = math_subset.train_test_split(test_size=val_size_fraction, seed=42)\n",
    "    math_train = math_splits[\"train\"]\n",
    "    math_val = math_splits[\"test\"]\n",
    "\n",
    "    print(f\"Tokenizing and filtering MATH...\")\n",
    "    math_train_tokenized = math_train.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    math_val_tokenized = math_val.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    print(f\"MATH: {len(math_train_tokenized)} train, {len(math_val_tokenized)} val (after filtering)\")\n",
    "\n",
    "    # # --- Experiment 1: Joint Training (Control Group) ---\n",
    "    # print(f\"\\n--- Starting Experiment 1: Joint Training ---\")\n",
    "\n",
    "    # if os.path.exists(os.path.join(JOINT_ADAPTER_PATH, \"adapter_model.safetensors\")):\n",
    "    #     print(f\"--- Found existing Joint adapter. Loading from {JOINT_ADAPTER_PATH} ---\")\n",
    "    #     joint_model = PeftModel.from_pretrained(base_model, JOINT_ADAPTER_PATH)\n",
    "    #     print(\"Adapter loaded successfully.\")\n",
    "\n",
    "    # else:\n",
    "    #     print(f\"--- No Joint adapter found. Starting Joint Training ---\")\n",
    "    #     lora_config = get_lora_config()\n",
    "    #     joint_model = get_peft_model(base_model, lora_config)\n",
    "    #     joint_model.print_trainable_parameters()\n",
    "\n",
    "    #     joint_train_dataset = concatenate_datasets([hotpot_train_tokenized, math_train_tokenized]).shuffle(seed=42)\n",
    "\n",
    "    #     joint_training_args = TrainingArguments(\n",
    "    #         output_dir=os.path.join(RESULTS_DIR, \"joint_training_temp\"),\n",
    "    #         per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    #         gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    #         num_train_epochs=JOINT_EPOCHS,\n",
    "    #         learning_rate=2e-4,\n",
    "    #         # fp16=True, # <-- 【BUG 修复】删除此行\n",
    "    #         logging_steps=50,\n",
    "    #         save_strategy=\"no\",\n",
    "    #         report_to=\"none\",\n",
    "    #         gradient_checkpointing=True,\n",
    "    #     )\n",
    "\n",
    "    #     joint_trainer = Trainer(\n",
    "    #         model=joint_model,\n",
    "    #         args=joint_training_args,\n",
    "    #         train_dataset=joint_train_dataset,\n",
    "    #         data_collator=data_collator,\n",
    "    #     )\n",
    "\n",
    "    #     joint_trainer.train()\n",
    "\n",
    "    #     print(f\"--- Joint training complete. Saving adapter to {JOINT_ADAPTER_PATH} ---\")\n",
    "    #     joint_model.save_pretrained(JOINT_ADAPTER_PATH)\n",
    "    #     print(\"Adapter saved.\")\n",
    "\n",
    "    #     del joint_trainer\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    # # --- Evaluate the \"Joint\" model (whether trained or loaded) ---\n",
    "    # print(\"\\n--- Evaluating Joint Model ---\")\n",
    "\n",
    "    # eval_args_joint = TrainingArguments(\n",
    "    #     output_dir=os.path.join(RESULTS_DIR, \"eval_temp_joint\"),\n",
    "    #     per_device_eval_batch_size=PER_DEVICE_BS,\n",
    "    #     # fp16=True, # <-- 【BUG 修复】删除此行\n",
    "    #     report_to=\"none\",\n",
    "    #     gradient_checkpointing=True,\n",
    "    # )\n",
    "\n",
    "    # eval_trainer_joint = Trainer(\n",
    "    #     model=joint_model,\n",
    "    #     args=eval_args_joint,\n",
    "    #     data_collator=data_collator,\n",
    "    # )\n",
    "\n",
    "    # eval_hotpot_joint = eval_trainer_joint.evaluate(eval_dataset=hotpot_val_tokenized)\n",
    "    # print(f\"  > Joint Model - HotpotQA Val Loss: {eval_hotpot_joint['eval_loss']:.4f}\")\n",
    "\n",
    "    # eval_math_joint = eval_trainer_joint.evaluate(eval_dataset=math_val_tokenized)\n",
    "    # print(f\"  > Joint Model - MATH Val Loss: {eval_math_joint['eval_loss']:.4f}\")\n",
    "\n",
    "    # del joint_model, eval_trainer_joint, eval_args_joint\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # --- 【已反转】Experiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---\n",
    "    print(f\"\\n--- Starting Experiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---\")\n",
    "\n",
    "    # --- Phase 1: Train on MATH (or load from checkpoint) ---\n",
    "    if os.path.exists(os.path.join(TASK_B_ADAPTER_PATH, \"adapter_model.safetensors\")):\n",
    "        print(f\"--- Found existing Task B (MATH) adapter. Loading from {TASK_B_ADAPTER_PATH} ---\")\n",
    "        seq_model = PeftModel.from_pretrained(base_model, TASK_B_ADAPTER_PATH)\n",
    "        print(\"Adapter loaded successfully.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"--- No adapter found. Starting Phase 1: Training on Task B (MATH) ---\")\n",
    "        lora_config = get_lora_config()\n",
    "        seq_model = get_peft_model(base_model, lora_config)\n",
    "        seq_model.print_trainable_parameters()\n",
    "\n",
    "        seq_args_b = TrainingArguments(\n",
    "            output_dir=os.path.join(RESULTS_DIR, \"seq_training_B_temp\"),\n",
    "            per_device_train_batch_size=PER_DEVICE_BS,\n",
    "            gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "            num_train_epochs=TASK_B_EPOCHS,\n",
    "            learning_rate=2e-4,\n",
    "            # fp16=True, # <-- 【BUG 修复】删除此行\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            gradient_checkpointing=True,\n",
    "        )\n",
    "\n",
    "        seq_trainer_b = Trainer(\n",
    "            model=seq_model,\n",
    "            args=seq_args_b,\n",
    "            train_dataset=math_train_tokenized, # <-- 训练 MATH\n",
    "            eval_dataset=math_val_tokenized,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        seq_trainer_b.train()\n",
    "\n",
    "        print(f\"--- Phase 1 (MATH) training complete. Saving adapter to {TASK_B_ADAPTER_PATH} ---\")\n",
    "        seq_model.save_pretrained(TASK_B_ADAPTER_PATH)\n",
    "        print(\"Adapter saved.\")\n",
    "\n",
    "        del seq_trainer_b\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "     # --- Evaluate the \"Task B Expert\" model (whether trained or loaded) ---\n",
    "    print(\"\\n--- Evaluating Model after Phase 1 (Task B Expert) ---\")\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=os.path.join(RESULTS_DIR, \"eval_temp\"),\n",
    "        per_device_eval_batch_size=PER_DEVICE_BS,\n",
    "        # fp16=True, # <-- 【BUG 修复】删除此行\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_trainer = Trainer(\n",
    "        model=seq_model,\n",
    "        args=eval_args,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    eval_hotpot_phase1 = eval_trainer.evaluate(eval_dataset=hotpot_val_tokenized)\n",
    "    print(f\"  > Task B Expert - HotpotQA Val Loss: {eval_hotpot_phase1['eval_loss']:.4f}\")\n",
    "    eval_math_phase1 = eval_trainer.evaluate(eval_dataset=math_val_tokenized)\n",
    "    print(f\"  > Task B Expert - MATH Val Loss: {eval_math_phase1['eval_loss']:.4f}\")\n",
    "    del eval_trainer, eval_args\n",
    "    torch.cuda.empty_cache()\n",
    "    # --- Phase 2: Train on HotpotQA (Forgetting MATH happens here) ---\n",
    "    print(f\"\\n  --- Phase 2: Training on Task A (HotpotQA) ---\")\n",
    "    history = {\"steps\": [], \"hotpot_loss\": [], \"math_loss\": []}\n",
    "    # Custom Trainer to log forgetting\n",
    "    class ForgettingTrackerCallback(TrainerCallback):\n",
    "      def __init__(self, hotpot_val, math_val, history_log, start_metrics):\n",
    "          super().__init__()\n",
    "          self.hotpot_eval_dataset = hotpot_val\n",
    "          self.math_eval_dataset = math_val\n",
    "          self.history = history_log\n",
    "          self.trainer = None\n",
    "          # --- 【修复】---\n",
    "          # 添加一个 \"锁\" 来防止无限递归\n",
    "          self.is_evaluating = False\n",
    "          # ----------------\n",
    "          # 记录初始状态 (Step 0)\n",
    "          self.history[\"steps\"].append(0)\n",
    "          self.history[\"hotpot_loss\"].append(start_metrics['hotpot_loss'])\n",
    "          self.history[\"math_loss\"].append(start_metrics['math_loss'])\n",
    "          print(\"Initializing ForgettingTrackerCallback with starting metrics.\")\n",
    "      def set_trainer(self, trainer):\n",
    "          \"\"\"在 Trainer 例化后, 注入对它的引用。\"\"\"\n",
    "          self.trainer = trainer\n",
    "          print(\"Trainer reference set in callback.\")\n",
    "\n",
    "      def on_log(self, args, state, control, **kwargs):\n",
    "          \"\"\"在 'logging_steps' 触发时被调用。\"\"\"\n",
    "          # --- 【修复 1】---\n",
    "          # 如果我们已经在这个函数中 (因为递归调用), 立即退出。\n",
    "          if self.is_evaluating:\n",
    "              return\n",
    "          # --- 【修复 2】---\n",
    "          # \"获取\" 锁\n",
    "          self.is_evaluating = True\n",
    "          # 确保 trainer 引用已被设置\n",
    "          if not self.trainer:\n",
    "              print(\"WARNING: Trainer reference not set in callback, skipping eval.\")\n",
    "              self.is_evaluating = False # <-- 别忘了在这里释放锁\n",
    "              return\n",
    "          print(f\"\\n--- Custom Eval at Step {state.global_step} ---\")\n",
    "          print(\"Evaluating on Task A (HotpotQA)...\")\n",
    "          # 使用 trainer 的 evaluate 方法\n",
    "          hotpot_metrics = self.trainer.evaluate(eval_dataset=self.hotpot_eval_dataset)\n",
    "          hotpot_loss = hotpot_metrics['eval_loss']\n",
    "          print(f\"  > Step {state.global_step} - HotpotQA Val Loss: {hotpot_loss:.4f} (LEARNING?)\")\n",
    "          print(\"Evaluating on Task B (MATH)...\")\n",
    "          math_metrics = self.trainer.evaluate(eval_dataset=self.math_eval_dataset)\n",
    "          math_loss = math_metrics['eval_loss']\n",
    "          print(f\"  > Step {state.global_step} - MATH Val Loss: {math_loss:.4f} (FORGETTING?)\")\n",
    "          self.history[\"steps\"].append(state.global_step)\n",
    "          self.history[\"hotpot_loss\"].append(hotpot_loss)\n",
    "          self.history[\"math_loss\"].append(math_loss)\n",
    "          # --- 【修复 3】---\n",
    "          # \"释放\" 锁, 以便下一次 on_log 可以运行\n",
    "          self.is_evaluating = False\n",
    "          self.trainer.model.train()\n",
    "\n",
    "\n",
    "    seq_args_a = TrainingArguments(\n",
    "        output_dir=os.path.join(RESULTS_DIR, \"seq_training_A\"),\n",
    "        per_device_train_batch_size=PER_DEVICE_BS,\n",
    "        gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "        num_train_epochs=TASK_A_EPOCHS,\n",
    "        learning_rate=7e-5,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[],         # <-- 保持这个设置\n",
    "        # disable_tqdm=True,  # <-- 保持这个设置\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    seq_model.enable_input_require_grads()\n",
    "    # 【修复 2】: 实例化 *新* 的 Callback\n",
    "    tracker_callback = ForgettingTrackerCallback(\n",
    "        hotpot_val=hotpot_val_tokenized,\n",
    "        math_val=math_val_tokenized,\n",
    "        history_log=history,\n",
    "        start_metrics={\n",
    "            'hotpot_loss': eval_hotpot_phase1['eval_loss'],\n",
    "            'math_loss': eval_math_phase1['eval_loss'],\n",
    "        }\n",
    "    )\n",
    "    # 【修复 3】: 实例化一个 *标准* Trainer, 并传入回调\n",
    "    seq_trainer_a = Trainer(\n",
    "        model=seq_model,\n",
    "        args=seq_args_a,\n",
    "        train_dataset=hotpot_train_tokenized,\n",
    "        eval_dataset=hotpot_val_tokenized,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[tracker_callback]  # <-- 在这里传入回调\n",
    "    )\n",
    "    # 【修复 4】: 将 trainer 实例链接回回调\n",
    "    # (回调需要这个引用来调用 self.trainer.evaluate())\n",
    "    tracker_callback.set_trainer(seq_trainer_a)\n",
    "    seq_trainer_a.train()\n",
    "\n",
    "    # --- 5. Plot Results ---\n",
    "    print(\"\\n--- Saving History Data and Generating Plot ---\")\n",
    "\n",
    "    # --- 保存 history data 到 JSON ---\n",
    "    history_filename = os.path.join(RESULTS_DIR, \"forgetting_history_MATH_to_HotpotQA_fp32.json\")\n",
    "    try:\n",
    "        with open(history_filename, 'w') as f:\n",
    "            json.dump(history, f, indent=4)\n",
    "        print(f\"History data saved to {history_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history to JSON: {e}\")\n",
    "    # --- [END] ---\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history[\"steps\"], history[\"hotpot_loss\"], 'o-', label=\"Task A (HotpotQA) Loss\", color=\"blue\")\n",
    "    plt.plot(history[\"steps\"], history[\"math_loss\"], 'o-', label=\"Task B (MATH) Loss\", color=\"red\")\n",
    "\n",
    "    plt.title(f\"Catastrophic Forgetting: MATH -> HotpotQA (Model: {MODEL_NAME} FP32 LoRA)\")\n",
    "    plt.xlabel(f\"Training Steps on Task A (HotpotQA) (Total Epochs: {TASK_A_EPOCHS})\")\n",
    "    plt.ylabel(\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_filename = os.path.join(RESULTS_DIR, \"sequential_forgetting_curve_MATH_to_HotpotQA_fp32.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Plot saved to {plot_filename}\")\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Not in Colab, plot saved to file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: This experiment requires a GPU. Check Colab runtime type.\")\n",
    "    else:\n",
    "        print(f\"INFO: Running on GPU. VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        if torch.cuda.get_device_properties(0).total_memory / 1e9 < 11:\n",
    "            print(\"WARNING: VRAM is less than 11GB. You may hit OOM errors. Try lowering MAX_SEQ_LENGTH.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
